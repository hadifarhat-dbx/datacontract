{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c466317-fef8-422b-9f62-78374eb90be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# datacontract-cli — Unity Catalog demo (v2)\n",
    "\n",
    "- Uses `dataContractSpecification: 1.2.0` (supported schema). soda-core==3.3.20\n",
    "- Robust YAML path resolution (works in Repos **and** Users workspace).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce047cb-f13b-4d4d-9f15-a1ee57a2684d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install datacontract-cli"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U pip\n",
    "%pip install --force-reinstall --no-cache-dir 'datacontract-cli[databricks]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bdb834-7706-4fcd-ad14-ca1051716e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83fe3ff3-855b-4119-91b8-b2e88bf8f6b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test CLI installation, input and output"
    }
   },
   "outputs": [],
   "source": [
    "#Quick test\n",
    "!datacontract --version\n",
    "!datacontract test https://datacontract.com/examples/orders-latest/datacontract.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346a4590-b458-4ece-9cfe-026802667b4c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create catalog, schema and tables and volumes configs"
    }
   },
   "outputs": [],
   "source": [
    "# Config for UC targets\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    CATALOG: str = os.getenv(\"DC_DEMO_CATALOG\", \"DATAC\")\n",
    "    SCHEMA: str = os.getenv(\"DC_DEMO_SCHEMA\", \"orders\")\n",
    "    TABLE: str = \"orders_raw\"\n",
    "    VOLUME_PATH: str = f\"/Volumes/{CATALOG}/{SCHEMA}/contract_audit/\"\n",
    "\n",
    "cfg = Cfg()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4904764-300e-48e3-9075-5cd779d73ffc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set local yaml path"
    }
   },
   "outputs": [],
   "source": [
    "YAML_PATH = \"/Workspace/Users/hadi.farhat@databricks.com/datacontract/datacontracts/orders/datacontract.uc2.yaml\"\n",
    "print(\"YAML:\", YAML_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bbb30fd-c673-4b8f-8cd4-2977130e3462",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Demo Data"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# --- setup catalog & schema, reset target table ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {cfg.CATALOG}.{cfg.SCHEMA}\")\n",
    "spark.sql(f\"USE CATALOG {cfg.CATALOG}\")\n",
    "spark.sql(f\"USE {cfg.SCHEMA}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {cfg.CATALOG}.{cfg.SCHEMA}.{cfg.TABLE}\")\n",
    "\n",
    "now = datetime.datetime.utcnow()\n",
    "\n",
    "# Helper: EUR → cents (long)\n",
    "def cents(x: float) -> int:\n",
    "    return int(round(x * 100))\n",
    "\n",
    "rows_ok = [\n",
    "    # order_id, order_timestamp ISO,           order_total_cents, customer_id,  customer_email,    processed_timestamp ISO\n",
    "    (\"o-1\",     now.isoformat(),               cents(19.99),      \"1000000001\",\"a@x.com\",          (now + timedelta(minutes=1)).isoformat()),\n",
    "    (\"o-2\",     (now - timedelta(hours=2)).isoformat(), cents(29.99), \"1000000002\",\"b@x.com\",    (now + timedelta(minutes=1)).isoformat()),\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.StringType(), False),\n",
    "    T.StructField(\"order_timestamp\", T.StringType(), False),\n",
    "    T.StructField(\"order_total\", T.LongType(),  False),   # long in cents\n",
    "    T.StructField(\"customer_id\", T.StringType(), True),\n",
    "    T.StructField(\"customer_email_address\", T.StringType(), False),\n",
    "    T.StructField(\"processed_timestamp\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_ok = (\n",
    "    spark.createDataFrame(rows_ok, schema)\n",
    "         .withColumn(\"order_timestamp\", F.to_timestamp(\"order_timestamp\"))\n",
    "         .withColumn(\"processed_timestamp\", F.to_timestamp(\"processed_timestamp\"))\n",
    ")\n",
    "\n",
    "# --- write to UC as delta ---\n",
    "target_fq = f\"{cfg.CATALOG}.{cfg.SCHEMA}.{cfg.TABLE}\"  # e.g., DATAC.orders.orders_raw\n",
    "(df_ok.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(target_fq))\n",
    "\n",
    "# --- temp view matches the model key in the contract (e.g., 'orders_raw') ---\n",
    "spark.table(target_fq).createOrReplaceTempView(cfg.TABLE)\n",
    "print(f\"Temp view created -> {cfg.TABLE} => {target_fq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53fca796-d756-4b69-8821-331ce6b22495",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check created table"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\" SELECT * FROM {cfg.CATALOG}.{cfg.SCHEMA}.{cfg.TABLE}\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca3d446-1603-4f98-a0b4-9c703a3e771b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Test and Write to View"
    }
   },
   "outputs": [],
   "source": [
    "from datacontract.data_contract import DataContract\n",
    "import re\n",
    "\n",
    "# --- Run lint & test ---\n",
    "dc = DataContract(data_contract_file=YAML_PATH, spark=spark)\n",
    "\n",
    "lint = dc.lint()\n",
    "print(f\"Lint result: {lint.result}\")\n",
    "\n",
    "run = dc.test()\n",
    "\n",
    "# --- Helper: parse checks into rows ---\n",
    "def _enum_to_str(v):\n",
    "    s = str(v)\n",
    "    m = re.search(r\"'([^']+)'\", s)\n",
    "    return m.group(1) if m else s\n",
    "\n",
    "def checks_to_rows(run_dict):\n",
    "    rows = []\n",
    "    for c in run_dict.get(\"checks\", []):\n",
    "        diag = c.get(\"diagnostics\", {}) or {}\n",
    "        fail = diag.get(\"fail\") or {}\n",
    "        thr_parts = []\n",
    "        if \"lessThan\" in fail and \"greaterThan\" in fail:\n",
    "            thr_parts.append(f\"between {fail['greaterThan']} and {fail['lessThan']}\")\n",
    "        else:\n",
    "            if \"lessThan\" in fail:\n",
    "                thr_parts.append(f\"< {fail['lessThan']}\")\n",
    "            if \"lessThanOrEqual\" in fail:\n",
    "                thr_parts.append(f\"≤ {fail['lessThanOrEqual']}\")\n",
    "            if \"greaterThan\" in fail:\n",
    "                thr_parts.append(f\"> {fail['greaterThan']}\")\n",
    "            if \"greaterThanOrEqual\" in fail:\n",
    "                thr_parts.append(f\"≥ {fail['greaterThanOrEqual']}\")\n",
    "        thresholds = \", \".join(thr_parts) or \"\"\n",
    "        rows.append((\n",
    "            _enum_to_str(c.get(\"result\")).replace(\"ResultEnum.\", \"\").lower(),\n",
    "            c.get(\"category\"),\n",
    "            c.get(\"model\"),\n",
    "            c.get(\"field\") or \"\",\n",
    "            c.get(\"name\"),\n",
    "            diag.get(\"value\", \"\"),\n",
    "            thresholds\n",
    "        ))\n",
    "    return rows\n",
    "\n",
    "# --- Extract dict form of run ---\n",
    "try:\n",
    "    run_dict = run.model_dump()\n",
    "except Exception:\n",
    "    try:\n",
    "        run_dict = run.__dict__\n",
    "    except Exception:\n",
    "        run_dict = vars(run)\n",
    "\n",
    "rows = checks_to_rows(run_dict)\n",
    "\n",
    "# --- Save to Delta ---\n",
    "result_schema = [\"Result\", \"Category\", \"Model\", \"Field\", \"Check\", \"Measured\", \"Thresholds\"]\n",
    "\n",
    "df_results = spark.createDataFrame(rows, result_schema)\n",
    "\n",
    "target_results_table = f\"{cfg.CATALOG}.{cfg.SCHEMA}.orders_quality_results\"\n",
    "\n",
    "(df_results.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(target_results_table))\n",
    "\n",
    "print(f\"✅ Results saved to {target_results_table}\")\n",
    "\n",
    "# --- Optionally wrap in a materialized view (Databricks SQL syntax) ---\n",
    "mv_name = f\"{cfg.CATALOG}.{cfg.SCHEMA}.orders_quality_mv\"\n",
    "spark.sql(f\"DROP VIEW IF EXISTS v_orders_audit\")\n",
    "spark.sql(f\"CREATE VIEW v_orders_audit AS SELECT * FROM {target_results_table}\")\n",
    "\n",
    "print(f\"📊 Materialized view created: v_orders_audit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008c78a7-a4e8-43b6-b12d-f5365dc4a3ae",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759321501317}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Display results"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {cfg.CATALOG}.{cfg.SCHEMA}.v_orders_audit\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "datacontract_uc_demo_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
